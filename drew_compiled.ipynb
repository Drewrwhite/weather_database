{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "\n",
    "city_file = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/cities.json\"\n",
    "\n",
    "with open(city_file) as f:\n",
    "    cities = json.load(f)\n",
    "\n",
    "data = []\n",
    "\n",
    "for city in cities:\n",
    "    response = requests.get(city['NWS_URL'])\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    location = soup.find('h2', {'class': 'panel-title'})\n",
    "    lat_lon_elev = soup.find('span', {'class': 'smallTxt'}).text.strip()\n",
    "    lat, lon, elev = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', lat_lon_elev)\n",
    "    temperature = soup.find('p', {'class': 'myforecast-current-lrg'})\n",
    "    humidity_elem = soup.find('td', text='Humidity')\n",
    "    humidity = humidity_elem.find_next('td').text.strip() if humidity_elem else 'NA'\n",
    "    wind_speed_elem = soup.find('td', text='Wind Speed')\n",
    "    wind_speed = wind_speed_elem.find_next('td').text.strip() if wind_speed_elem else 'NA'\n",
    "    barometer_elem = soup.find('td', text='Barometer')\n",
    "    barometer = barometer_elem.find_next('td').text.strip() if barometer_elem else 'NA'\n",
    "    dewpoint_elem = soup.find('td', text='Dewpoint')\n",
    "    dewpoint = dewpoint_elem.find_next('td').text.strip() if dewpoint_elem else 'NA'\n",
    "    visibility_elem = soup.find('td', text='Visibility')\n",
    "    visibility = visibility_elem.find_next('td').text.strip() if visibility_elem else 'NA'\n",
    "    wind_chill_elem = soup.find('td', text='Wind Chill')\n",
    "    wind_chill = wind_chill_elem.find_next('td').text.strip() if wind_chill_elem else 'NA'\n",
    "    last_update_elem = soup.find('td', text='Last update')\n",
    "    last_update = last_update_elem.find_next('td').text.strip() if last_update_elem else 'NA'\n",
    "\n",
    "    data.append({\n",
    "        'location': city['Name'],\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'elev_ft': elev,\n",
    "        'temperature': temperature.text if temperature else 'NA',\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'barometer': barometer,\n",
    "        'dewpoint': dewpoint,\n",
    "        'vis_miles': visibility,\n",
    "        'wind_chill': wind_chill,\n",
    "        'last_update': last_update\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the 'location' column into separate 'city' and 'state' columns\n",
    "df[['city', 'state']] = df['location'].str.split(', ', expand=True)\n",
    "\n",
    "# Convert 'lat' and 'lon' columns to float type\n",
    "df[['lat', 'lon']] = df[['lat', 'lon']].astype(float)\n",
    "\n",
    "# Convert 'elev' column to int type\n",
    "df['elev_ft'] = df['elev_ft'].astype(int)\n",
    "\n",
    "# Extract the numeric part of the temperature string and convert it to int\n",
    "df['temp_f'] = df['temperature'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Convert temperature to Celsius and add to new column 'temp_c'\n",
    "df['temp_c'] = (df['temp_f'] - 32) * 5/9\n",
    "\n",
    "# Round 'temp_c' to nearest integer and cast to int type\n",
    "df['temp_c'] = df['temp_c'].round().astype(int)\n",
    "\n",
    "# Convert 'humidity' column to float type\n",
    "df['humidity'] = df['humidity'].str.extract('(\\d+)', expand=False).astype(float) / 100\n",
    "\n",
    "# Split wind speed values into components and convert speed to int type\n",
    "df['wind_speed'] = df['wind_speed'].str.extract('(\\d+)', expand=False).fillna(0).astype(int)\n",
    "\n",
    "# Set any missing or non-numeric wind speed values to 0\n",
    "df['wind_speed'] = df['wind_speed'].replace('Calm', 0)\n",
    "\n",
    "# Convert 'barometer' column to float type, and convert inches to millibars\n",
    "df['barometer'] = df['barometer'].apply(lambda x: float(x.split()[0]) * 33.8639 if 'in' in x and x != 'NA' else None)\n",
    "\n",
    "# Round 'barometer' to two decimal places\n",
    "df['barometer'] = df['barometer'].round(2)\n",
    "\n",
    "# Split 'dewpoint' column into separate 'dewpoint_f' and 'dewpoint_c' columns\n",
    "df[['dewpoint_f', 'dewpoint_c']] = df['dewpoint'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(int)\n",
    "\n",
    "# Convert 'vis_miles' column to float type\n",
    "df['vis_miles'] = df['vis_miles'].str.extract('(\\d+\\.\\d+|\\d+)', expand=False).astype(float).round(2)\n",
    "\n",
    "# Split 'wind_chill' column into separate 'wind_chill_f' and 'wind_chill_c' columns\n",
    "df[['wind_chill_f', 'wind_chill_c']] = df['wind_chill'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(float)\n",
    "\n",
    "# Convert 'last_update' column to datetime type with the desired format and time zone\n",
    "df['last_update'] = df['last_update'].apply(lambda x: dateutil.parser.parse(x, tzinfos={'CST': dateutil.tz.tzoffset(None, -21600)}))\n",
    "\n",
    "# Convert 'last_update' column to UTC\n",
    "df['last_update'] = df['last_update'].apply(lambda x: x.astimezone(dateutil.tz.tzutc()))\n",
    "\n",
    "# Drop columns that were split into two values\n",
    "df = df.drop(['temperature', 'dewpoint', 'wind_chill'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Set up the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Set the project and dataset IDs\n",
    "project_id = \"deb-dev-dw\"\n",
    "dataset_id = \"weather\"\n",
    "table_id = \"daily\"\n",
    "\n",
    "# Set the table schema\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"elev_ft\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"humidity\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"wind_speed\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"barometer\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"vis_miles\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"dewpoint_c\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"wind_chill_c\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"temp_f\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"temp_c\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"last_update\", \"TIMESTAMP\"),\n",
    "]\n",
    "\n",
    "# Check if the dataset exists, and create it if it does not\n",
    "try:\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "except NotFound:\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"US\"\n",
    "    dataset = client.create_dataset(dataset)\n",
    "\n",
    "# Get a reference to the table\n",
    "table_ref = dataset.table(table_id)\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "try:\n",
    "    client.get_table(table_ref)\n",
    "except NotFound:\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "\n",
    "# Write the DataFrame to BigQuery\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "city_file = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/airflow/data/cities.json\"\n",
    "\n",
    "def scrape_weather_data(city_file):\n",
    "    with open(city_file) as f:\n",
    "        cities = json.load(f)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for city in cities:\n",
    "        response = requests.get(city['NWS_URL'])\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        location = soup.find('h2', {'class': 'panel-title'})\n",
    "        lat_lon_elev = soup.find('span', {'class': 'smallTxt'}).text.strip()\n",
    "        lat, lon, elev = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', lat_lon_elev)\n",
    "        temperature = soup.find('p', {'class': 'myforecast-current-lrg'})\n",
    "        humidity_elem = soup.find('td', text='Humidity')\n",
    "        humidity = humidity_elem.find_next('td').text.strip() if humidity_elem else 'NA'\n",
    "        wind_speed_elem = soup.find('td', text='Wind Speed')\n",
    "        wind_speed = wind_speed_elem.find_next('td').text.strip() if wind_speed_elem else 'NA'\n",
    "        barometer_elem = soup.find('td', text='Barometer')\n",
    "        barometer = barometer_elem.find_next('td').text.strip() if barometer_elem else 'NA'\n",
    "        dewpoint_elem = soup.find('td', text='Dewpoint')\n",
    "        dewpoint = dewpoint_elem.find_next('td').text.strip() if dewpoint_elem else 'NA'\n",
    "        visibility_elem = soup.find('td', text='Visibility')\n",
    "        visibility = visibility_elem.find_next('td').text.strip() if visibility_elem else 'NA'\n",
    "        wind_chill_elem = soup.find('td', text='Wind Chill')\n",
    "        wind_chill = wind_chill_elem.find_next('td').text.strip() if wind_chill_elem else 'NA'\n",
    "        last_update_elem = soup.find('td', text='Last update')\n",
    "        last_update = last_update_elem.find_next('td').text.strip() if last_update_elem else 'NA'\n",
    "\n",
    "        data.append({\n",
    "            'location': city['Name'],\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'elev_ft': elev,\n",
    "            'temperature': temperature.text if temperature else 'NA',\n",
    "            'humidity': humidity,\n",
    "            'wind_speed': wind_speed,\n",
    "            'barometer': barometer,\n",
    "            'dewpoint': dewpoint,\n",
    "            'vis_miles': visibility,\n",
    "            'wind_chill': wind_chill,\n",
    "            'last_update': last_update\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "scrape_weather_data(city_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather_data(df):\n",
    "\n",
    "    df[['city', 'state']] = df['location'].str.split(', ', expand=True)\n",
    "    df[['lat', 'lon']] = df[['lat', 'lon']].astype(float)\n",
    "    df['elev_ft'] = df['elev_ft'].astype(int)\n",
    "    df['temp_f'] = df['temperature'].str.extract('(\\d+)').astype(int)\n",
    "    df['temp_c'] = (df['temp_f'] - 32) * 5/9\n",
    "    df['temp_c'] = df['temp_c'].round().astype(int)\n",
    "    df['humidity'] = df['humidity'].str.extract('(\\d+)', expand=False).astype(float) / 100\n",
    "    df['wind_speed'] = df['wind_speed'].str.extract('(\\d+)', expand=False).fillna(0).astype(int)\n",
    "    df['wind_speed'] = df['wind_speed'].replace('Calm', 0)\n",
    "    df['barometer'] = df['barometer'].apply(lambda x: float(x.split()[0]) * 33.8639 if 'in' in x and x != 'NA' else None)\n",
    "    df['barometer'] = df['barometer'].round(2)\n",
    "    df[['dewpoint_f', 'dewpoint_c']] = df['dewpoint'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(int)\n",
    "    df['vis_miles'] = df['vis_miles'].str.extract('(\\d+\\.\\d+|\\d+)', expand=False).astype(float).round(2)\n",
    "    df[['wind_chill_f', 'wind_chill_c']] = df['wind_chill'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(float)\n",
    "    df['last_update'] = df['last_update'].apply(lambda x: dateutil.parser.parse(x, tzinfos={'CST': dateutil.tz.tzoffset(None, -21600)}))\n",
    "    df['last_update'] = df['last_update'].apply(lambda x: x.astimezone(dateutil.tz.tzutc()))\n",
    "    df = df.drop(['temperature', 'dewpoint', 'wind_chill'], axis=1)\n",
    "\n",
    "    return df\n",
    "transform_weather_data(scrape_weather_data(city_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weather_data_to_bigquery(df):\n",
    "\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    project_id = \"deb-dev-dw\"\n",
    "    dataset_id = \"weather\"\n",
    "    table_id = \"daily\"\n",
    "\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"elev_ft\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"humidity\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_speed\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"barometer\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"vis_miles\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"dewpoint_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"wind_chill_f\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_chill_c\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"temp_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"temp_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"last_update\", \"TIMESTAMP\"),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        dataset_ref = client.dataset(dataset_id)\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "    except NotFound:\n",
    "        dataset_ref = client.dataset(dataset_id)\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"\n",
    "        dataset = client.create_dataset(dataset)\n",
    "\n",
    "    table_ref = dataset.table(table_id)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "write_weather_data_to_bigquery(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import json\n",
    "from google.cloud.exceptions import NotFound\n",
    "from airflow import DAG\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from google.cloud import bigquery\n",
    "\n",
    "city_file = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/cities.json\"\n",
    "PROJECT_ID = \"deb-dev-dw\"\n",
    "DATASET_ID = \"weather\"\n",
    "DAILY_TABLE_ID = \"daily\"\n",
    "\n",
    "SCHEMA = [\n",
    "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"elev_ft\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"humidity\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_speed\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"barometer\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"vis_miles\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"dewpoint_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"wind_chill_f\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_chill_c\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"temp_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"temp_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"last_update\", \"TIMESTAMP\"),\n",
    "    ]\n",
    "\n",
    "@task\n",
    "def scrape_weather_data():\n",
    "    with open(city_file) as f:\n",
    "        cities = json.load(f)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for city in cities:\n",
    "        try:\n",
    "            response = requests.get(city['NWS_URL'])\n",
    "        except:\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        location = soup.find('h2', {'class': 'panel-title'})\n",
    "        lat_lon_elev = soup.find('span', {'class': 'smallTxt'}).text.strip()\n",
    "        lat, lon, elev = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', lat_lon_elev)\n",
    "        temperature = soup.find('p', {'class': 'myforecast-current-lrg'})\n",
    "        humidity_elem = soup.find('td', text='Humidity')\n",
    "        humidity = humidity_elem.find_next('td').text.strip() if humidity_elem else 'NA'\n",
    "        wind_speed_elem = soup.find('td', text='Wind Speed')\n",
    "        wind_speed = wind_speed_elem.find_next('td').text.strip() if wind_speed_elem else 'NA'\n",
    "        barometer_elem = soup.find('td', text='Barometer')\n",
    "        barometer = barometer_elem.find_next('td').text.strip() if barometer_elem else 'NA'\n",
    "        dewpoint_elem = soup.find('td', text='Dewpoint')\n",
    "        dewpoint = dewpoint_elem.find_next('td').text.strip() if dewpoint_elem else 'NA'\n",
    "        visibility_elem = soup.find('td', text='Visibility')\n",
    "        visibility = visibility_elem.find_next('td').text.strip() if visibility_elem else 'NA'\n",
    "        wind_chill_elem = soup.find('td', text='Wind Chill')\n",
    "        wind_chill = wind_chill_elem.find_next('td').text.strip() if wind_chill_elem else 'NA'\n",
    "        last_update_elem = soup.find('td', text='Last update')\n",
    "        last_update = last_update_elem.find_next('td').text.strip() if last_update_elem else 'NA'\n",
    "\n",
    "        data.append({\n",
    "            'location': city['Name'],\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'elev_ft': elev,\n",
    "            'temperature': temperature.text if temperature else 'NA',\n",
    "            'humidity': humidity,\n",
    "            'wind_speed': wind_speed,\n",
    "            'barometer': barometer,\n",
    "            'dewpoint': dewpoint,\n",
    "            'vis_miles': visibility,\n",
    "            'wind_chill': wind_chill,\n",
    "            'last_update': last_update\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def transform_weather_data(df):\n",
    "    transformations = [\n",
    "        (['city', 'state'], df['location'].str.split(', ', expand=True)),\n",
    "        (['lat', 'lon'], df[['lat', 'lon']].astype(float)),\n",
    "        ('elev_ft', df['elev_ft'].astype(int)),\n",
    "        ('temp_f', df['temperature'].str.extract('(\\d+)').astype(int)),\n",
    "        ('temp_c', (df['temp_f'] - 32) * 5/9),\n",
    "        ('temp_c', df['temp_c'].round().astype(int)),\n",
    "        ('humidity', df['humidity'].str.extract('(\\d+)', expand=False).astype(float) / 100),\n",
    "        ('wind_speed', df['wind_speed'].str.extract('(\\d+)', expand=False).fillna(0).astype(int)),\n",
    "        ('wind_speed', df['wind_speed'].replace('Calm', 0)),\n",
    "        ('barometer', df['barometer'].apply(lambda x: float(x.split()[0]) * 33.8639 if 'in' in x and x != 'NA' else None)),\n",
    "        ('barometer', df['barometer'].round(2)),\n",
    "        (['dewpoint_f', 'dewpoint_c'], df['dewpoint'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(int)),\n",
    "        ('vis_miles', df['vis_miles'].str.extract('(\\d+\\.\\d+|\\d+)', expand=False).astype(float).round(2)),\n",
    "        ('wind_chill_f', df['wind_chill'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(float)),\n",
    "        ('wind_chill_c', (df['wind_chill_f'] - 32) * 5/9),\n",
    "        ('wind_chill_c', df['wind_chill_c'].round().astype(int)),\n",
    "        ('last_update', df['last_update'].apply(lambda x: dateutil.parser.parse(x, tzinfos={'CST': dateutil.tz.tzoffset(None, -21600)}).astimezone(dateutil.tz.tzutc()))),\n",
    "    ]\n",
    "    for col, transform in transformations:\n",
    "        if isinstance(col, list):\n",
    "            df[col] = transform\n",
    "        else:\n",
    "            df[col] = transform\n",
    "\n",
    "    # Drop columns that were split into two values\n",
    "    df = df.drop(['temperature', 'dewpoint', 'wind_chill'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def write_weather_data_to_bigquery(df):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    try:\n",
    "        dataset_ref = client.dataset(DATASET_ID)\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "    except NotFound:\n",
    "        dataset_ref = client.dataset(DATASET_ID)\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"\n",
    "        dataset = client.create_dataset(dataset)\n",
    "\n",
    "    table_ref = dataset.table(DAILY_TABLE_ID)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=SCHEMA)\n",
    "        table = client.create_table(table)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "@dag(\n",
    "    'weather_data_pipeline',\n",
    "    description='Scrapes National Weather Service website every 12 hours, transforms data and loads to bigquery',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='0 0,12 * * *',\n",
    ")\n",
    "def weather_data_pipeline():\n",
    "\n",
    "    scrape_weather_data_task = scrape_weather_data()\n",
    "\n",
    "\n",
    "    transform_weather_data_task = transform_weather_data()\n",
    "\n",
    "\n",
    "    write_weather_data_to_bigquery_task = write_weather_data_to_bigquery()\n",
    "\n",
    "\n",
    "    done = EmptyOperator(task_id='done')\n",
    "\n",
    "    scrape_weather_data_task >> transform_weather_data_task >> write_weather_data_to_bigquery_task >> done\n",
    "\n",
    "dag = weather_data_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import json\n",
    "from google.cloud.exceptions import NotFound\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from google.cloud import bigquery\n",
    "\n",
    "city_file = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/cities.json\"\n",
    "\n",
    "def scrape_weather_data(city_file):\n",
    "    with open(city_file) as f:\n",
    "        cities = json.load(f)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for city in cities:\n",
    "        response = requests.get(city['NWS_URL'])\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        location = soup.find('h2', {'class': 'panel-title'})\n",
    "        lat_lon_elev = soup.find('span', {'class': 'smallTxt'}).text.strip()\n",
    "        lat, lon, elev = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', lat_lon_elev)\n",
    "        temperature = soup.find('p', {'class': 'myforecast-current-lrg'})\n",
    "        humidity_elem = soup.find('td', text='Humidity')\n",
    "        humidity = humidity_elem.find_next('td').text.strip() if humidity_elem else 'NA'\n",
    "        wind_speed_elem = soup.find('td', text='Wind Speed')\n",
    "        wind_speed = wind_speed_elem.find_next('td').text.strip() if wind_speed_elem else 'NA'\n",
    "        barometer_elem = soup.find('td', text='Barometer')\n",
    "        barometer = barometer_elem.find_next('td').text.strip() if barometer_elem else 'NA'\n",
    "        dewpoint_elem = soup.find('td', text='Dewpoint')\n",
    "        dewpoint = dewpoint_elem.find_next('td').text.strip() if dewpoint_elem else 'NA'\n",
    "        visibility_elem = soup.find('td', text='Visibility')\n",
    "        visibility = visibility_elem.find_next('td').text.strip() if visibility_elem else 'NA'\n",
    "        wind_chill_elem = soup.find('td', text='Wind Chill')\n",
    "        wind_chill = wind_chill_elem.find_next('td').text.strip() if wind_chill_elem else 'NA'\n",
    "        last_update_elem = soup.find('td', text='Last update')\n",
    "        last_update = last_update_elem.find_next('td').text.strip() if last_update_elem else 'NA'\n",
    "\n",
    "        data.append({\n",
    "            'location': city['Name'],\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'elev_ft': elev,\n",
    "            'temperature': temperature.text if temperature else 'NA',\n",
    "            'humidity': humidity,\n",
    "            'wind_speed': wind_speed,\n",
    "            'barometer': barometer,\n",
    "            'dewpoint': dewpoint,\n",
    "            'vis_miles': visibility,\n",
    "            'wind_chill': wind_chill,\n",
    "            'last_update': last_update\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_weather_data(df):\n",
    "    transformations = [\n",
    "        (['city', 'state'], df['location'].str.split(', ', expand=True)),\n",
    "        (['lat', 'lon'], df[['lat', 'lon']].astype(float)),\n",
    "        ('elev_ft', df['elev_ft'].astype(int)),\n",
    "        ('temp_f', df['temperature'].str.extract('(\\d+)').astype(int)),\n",
    "        ('temp_c', (df['temp_f'] - 32) * 5/9),\n",
    "        ('temp_c', df['temp_c'].round().astype(int)),\n",
    "        ('humidity', df['humidity'].str.extract('(\\d+)', expand=False).astype(float) / 100),\n",
    "        ('wind_speed', df['wind_speed'].str.extract('(\\d+)', expand=False).fillna(0).astype(int)),\n",
    "        ('wind_speed', df['wind_speed'].replace('Calm', 0)),\n",
    "        ('barometer', df['barometer'].apply(lambda x: float(x.split()[0]) * 33.8639 if 'in' in x and x != 'NA' else None)),\n",
    "        ('barometer', df['barometer'].round(2)),\n",
    "        (['dewpoint_f', 'dewpoint_c'], df['dewpoint'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(int)),\n",
    "        ('vis_miles', df['vis_miles'].str.extract('(\\d+\\.\\d+|\\d+)', expand=False).astype(float).round(2)),\n",
    "        (['wind_chill_f', 'wind_chill_c'], df['wind_chill'].str.extract('(\\d+).*?(\\d+)', expand=True).astype(float)),\n",
    "        ('last_update', df['last_update'].apply(lambda x: dateutil.parser.parse(x, tzinfos={'CST': dateutil.tz.tzoffset(None, -21600)}))),\n",
    "        ('last_update', df['last_update'].apply(lambda x: x.astimezone(dateutil.tz.tzutc()))),\n",
    "    ]\n",
    "    for col, transform in transformations:\n",
    "        if isinstance(col, list):\n",
    "            df[col] = transform\n",
    "        else:\n",
    "            df[col] = transform\n",
    "\n",
    "    # Drop columns that were split into two values\n",
    "    df = df.drop(['temperature', 'dewpoint', 'wind_chill'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_weather_data_to_bigquery():\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    project_id = \"deb-dev-dw\"\n",
    "    dataset_id = \"weather\"\n",
    "    table_id = \"daily\"\n",
    "\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"elev_ft\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"humidity\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_speed\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"barometer\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"vis_miles\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"dewpoint_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"wind_chill_f\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wind_chill_c\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"temp_f\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"temp_c\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"last_update\", \"TIMESTAMP\"),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        dataset_ref = client.dataset(dataset_id)\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "    except NotFound:\n",
    "        dataset_ref = client.dataset(dataset_id)\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"\n",
    "        dataset = client.create_dataset(dataset)\n",
    "\n",
    "    table_ref = dataset.table(table_id)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Drew White',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 2, 14),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'weather_data_pipeline',\n",
    "    description='Scrapes National Weather Service website every 12 hours, transforms data and loads to bigquery'\n",
    "    default_args=default_args,\n",
    "    schedule_interval='0 0,12 * * *',\n",
    ") as dag:\n",
    "\n",
    "scrape_weather_data_task = PythonOperator(\n",
    "    task_id='scrape_weather_data',\n",
    "    python_callable=scrape_weather_data\n",
    ")\n",
    "\n",
    "transform_weather_data_task = PythonOperator(\n",
    "    task_id='transform_weather_data',\n",
    "    python_callable=transform_weather_data\n",
    ")\n",
    "\n",
    "write_weather_data_to_bigquery_task = PythonOperator(\n",
    "    task_id='write_weather_data_to_bigquery',\n",
    "    python_callable=write_weather_data_to_bigquery\n",
    ")\n",
    "\n",
    "done = EmptyOperator(task_id='done')\n",
    "\n",
    "scrape_weather_data_task >> transform_weather_data_task >> write_weather_data_to_bigquery_task >> done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"deb-dev-dw\"\n",
    "DATASET_ID = \"weather\"\n",
    "DAILY_TABLE_ID = \"daily\"\n",
    "WEEKLY_TABLE_ID = \"weekly_avg\"\n",
    "\n",
    "\n",
    "def calculate_weekly_averages():\n",
    "    client = bigquery.Client()\n",
    "    query = f\"\"\"\n",
    "    SELECT location, city, state, lat, lon,\n",
    "        ROUND(AVG(temp_f), 1) AS temp_f_avg,\n",
    "        ROUND(AVG(temp_c), 1) AS temp_c_avg,\n",
    "        ROUND(AVG(humidity), 1) AS humidity_avg,\n",
    "        ROUND(AVG(barometer), 1) AS barometer_avg,\n",
    "        ROUND(AVG(dewpoint_f), 1) AS dewpoint_f_avg,\n",
    "        ROUND(AVG(dewpoint_c), 1) AS dewpoint_c_avg,\n",
    "        CURRENT_TIMESTAMP() AS modified_at\n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.{DAILY_TABLE_ID}`\n",
    "    WHERE DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) <= DATE(last_update)\n",
    "    GROUP BY location, city, state, lat, lon\n",
    "\"\"\"\n",
    "    weekly_df = client.query(query).to_dataframe()\n",
    "    return weekly_df\n",
    "\n",
    "print(calculate_weekly_averages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_weekly_averages_to_bigquery(df):\n",
    "    client = bigquery.Client()\n",
    "    project_id = \"deb-dev-dw\"\n",
    "    dataset_id = \"weather\"\n",
    "    table_id = \"weekly_avg\"\n",
    "\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"temp_f_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"temp_c_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"humidity_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"barometer_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"dewpoint_f_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"dewpoint_c_avg\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"modified_at\", \"TIMESTAMP\")\n",
    "    ]\n",
    "\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "write_weekly_averages_to_bigquery(calculate_weekly_averages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from google.cloud import bigquery\n",
    "from airflow import DAG\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python_operator import EmptyOperator\n",
    "\n",
    "PROJECT_ID = \"deb-dev-dw\"\n",
    "DATASET_ID = \"weather\"\n",
    "DAILY_TABLE_ID = \"daily\"\n",
    "WEEKLY_TABLE_ID = \"weekly_avg\"\n",
    "\n",
    "SCHEMA = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"city\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"state\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"lat\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"lon\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"temp_f_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"temp_c_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"humidity_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"barometer_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"dewpoint_f_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"dewpoint_c_avg\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"modified_at\", \"TIMESTAMP\")\n",
    "]\n",
    "\n",
    "@task\n",
    "def calculate_weekly_averages():\n",
    "    client = bigquery.Client()\n",
    "    query = f\"\"\"\n",
    "    SELECT location, city, state, lat, lon,\n",
    "        ROUND(AVG(temp_f), 1) AS temp_f_avg,\n",
    "        ROUND(AVG(temp_c), 1) AS temp_c_avg,\n",
    "        ROUND(AVG(humidity), 1) AS humidity_avg,\n",
    "        ROUND(AVG(barometer), 1) AS barometer_avg,\n",
    "        ROUND(AVG(dewpoint_f), 1) AS dewpoint_f_avg,\n",
    "        ROUND(AVG(dewpoint_c), 1) AS dewpoint_c_avg,\n",
    "        CURRENT_TIMESTAMP() AS modified_at\n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.{DAILY_TABLE_ID}`\n",
    "    WHERE DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) <= DATE(last_update)\n",
    "    GROUP BY location, city, state, lat, lon\n",
    "\"\"\"\n",
    "    weekly_df = client.query(query).to_dataframe()\n",
    "    return weekly_df\n",
    "\n",
    "@task\n",
    "def write_weekly_avg_to_bq(df, **context):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    table_ref = dataset_ref.table(WEEKLY_TABLE_ID)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=SCHEMA)\n",
    "        table = client.create_table(table)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "    # Create the BigQuery insert job operator\n",
    "    insert_job = BigQueryInsertJobOperator(\n",
    "        task_id='insert_weekly_avg_to_bq',\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_id=WEEKLY_TABLE_ID,\n",
    "        schema_fields=SCHEMA,\n",
    "        bigquery_conn_id='google_cloud_default',\n",
    "        time_partitioning={'type': 'DAY'},\n",
    "        use_legacy_sql=False,\n",
    "        template_suffix=f\"_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        task_concurrency=1,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "    # Execute the BigQuery insert job operator\n",
    "    insert_job.execute(context={\n",
    "        'ti': context['ti'],\n",
    "        'weekly_averages_df': df\n",
    "    })\n",
    "\n",
    "@dag(\n",
    "    schedule_interval=\"@weekly\",\n",
    "    description='Calculates weekly averages of weather data in bigquery',\n",
    "    start_date=datetime.utcnow(),\n",
    "    catchup=False,\n",
    "    default_view='graph',\n",
    "    is_paused_upon_creation=True,\n",
    "    tags=['averages', 'weekly averages'],\n",
    ")\n",
    "\n",
    "def weekly_avg():\n",
    "\n",
    "    calculate_weekly_avg_task = calculate_weekly_averages()\n",
    "\n",
    "    write_weekly_avg_task = write_weekly_avg_to_bq(calculate_weekly_averages)\n",
    "\n",
    "    insert_weekly_avg_to_bq_task = BigQueryInsertJobOperator(\n",
    "        task_id='insert_weekly_avg_to_bq',\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_id=WEEKLY_TABLE_ID,\n",
    "        schema_fields=SCHEMA,\n",
    "        bigquery_conn_id='google_cloud_default',\n",
    "        time_partitioning={'type': 'DAY'},\n",
    "        use_legacy_sql=False,\n",
    "        template_suffix=f\"_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        task_concurrency=1,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "    done = EmptyOperator(task_id='done')\n",
    "\n",
    "    calculate_weekly_avg_task >> write_weekly_avg_task >> insert_weekly_avg_to_bq_task >> done\n",
    "\n",
    "dag = write_weekly_avg_to_bq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_weather_data():\n",
    "    \"\"\"scrapes weather from national weather service and turns to json\n",
    "\n",
    "    Returns:\n",
    "        json of weather data\n",
    "    \"\"\"\n",
    "    CITIES = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/airflow/data/cities.json\"\n",
    "    with open(CITIES, 'r') as f:\n",
    "        CITIES = json.load(f)\n",
    "\n",
    "    data = []\n",
    "    # loop through cities using beautiful soup\n",
    "    for city in CITIES:\n",
    "        name = city['Name']\n",
    "        url = city['NWS_URL']\n",
    "\n",
    "    # Send a request to the NWS_URL and get the HTML response\n",
    "        response = requests.get(url)\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML response\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # scrape location\n",
    "        location = soup.find(\"h2\", {\"class\": \"panel-title\"})\n",
    "\n",
    "        # scrape lat, lon and elv\n",
    "        print()\n",
    "        lat_lon_elev = soup.find(\"span\", {\"class\": \"smallTxt\"}).text.strip()\n",
    "        lat, lon, elev = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", lat_lon_elev)\n",
    "\n",
    "        # scrape temperature\n",
    "        temperature = soup.find(\"p\", {\"class\": \"myforecast-current-lrg\"})\n",
    "\n",
    "        # scrape humidity\n",
    "        humidity_elem = soup.find(\"td\", text=\"Humidity\")\n",
    "        humidity = humidity_elem.find_next(\"td\").text.strip() if humidity_elem else None\n",
    "\n",
    "        # scrape wind speed\n",
    "        wind_speed_elem = soup.find(\"td\", text=\"Wind Speed\")\n",
    "        wind_speed = (wind_speed_elem.find_next(\"td\").text.strip() if wind_speed_elem else None)\n",
    "\n",
    "        # scrape barometer\n",
    "        barometer_elem = soup.find(\"td\", text=\"Barometer\")\n",
    "        barometer = (barometer_elem.find_next(\"td\").text.strip() if barometer_elem else None)\n",
    "\n",
    "        # scrape dew point\n",
    "        dewpoint_elem = soup.find(\"td\", text=\"Dewpoint\")\n",
    "        dewpoint = dewpoint_elem.find_next(\"td\").text.strip() if dewpoint_elem else None\n",
    "\n",
    "        # scrape visibility\n",
    "        visibility_elem = soup.find(\"td\", text=\"Visibility\")\n",
    "        visibility = (visibility_elem.find_next(\"td\").text.strip() if visibility_elem else None)\n",
    "\n",
    "        # scrape windchill\n",
    "        wind_chill_elem = soup.find(\"td\", text=\"Wind Chill\")\n",
    "        wind_chill = (wind_chill_elem.find_next(\"td\").text.strip() if wind_chill_elem else None)\n",
    "\n",
    "        # scrape last update date\n",
    "        last_update_elem = soup.find(\"td\", text=\"Last update\")\n",
    "        last_update = (last_update_elem.find_next(\"td\").text.strip() if last_update_elem else None)\n",
    "\n",
    "        # appends scraped data in following format:\n",
    "        data.append(\n",
    "            {\n",
    "                \"location\": city[\"Name\"],\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"elev_ft\": elev,\n",
    "                \"temperature\": temperature.text if temperature else None,\n",
    "                \"humidity\": humidity,\n",
    "                \"wind_speed\": wind_speed,\n",
    "                \"barometer\": barometer,\n",
    "                \"dewpoint\": dewpoint,\n",
    "                \"vis_miles\": visibility,\n",
    "                \"wind_chill\": wind_chill,\n",
    "                \"last_update\": last_update,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    data = df.to_json(orient=\"records\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "scrape_weather_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CITIES = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/airflow/data/cities.json\"\n",
    "combined_df = pd.DataFrame()\n",
    "with open(CITIES, 'r') as f:\n",
    "    CITIES = json.load(f)\n",
    "for city in CITIES:\n",
    "    soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "\n",
    "    #various containers\n",
    "    item1 = soup.find_all(id='current_conditions-summary')\n",
    "    item2 = soup.find_all(id='current_conditions_detail')\n",
    "    item4 = soup.find_all(id='tombstone-container')\n",
    "\n",
    "    #raw data\n",
    "    temp_f = [item.find(class_=\"myforecast-current-lrg\").get_text() for item in item1]\n",
    "    temp_min = soup.find('p', {'class': 'temp temp-low'}).text.strip()\n",
    "    temp_max = soup.find('p', {'class': 'temp temp-high'}).text.strip()\n",
    "\n",
    "\n",
    "    #df of temperatures\n",
    "    df_temperature = pd.DataFrame({\"temp\" : temp_f,'tempmin': temp_min,'tempmax': temp_max})\n",
    "\n",
    "    #df_2 is a df of current conditions in detail (Humidity, Wind Speed, Barometer, Dewpoint, Visibility, Last update)\n",
    "    table = soup.find_all('table')\n",
    "    df_2 = pd.read_html(str(table))[0]\n",
    "    df_2 = df_2.pivot(columns=0, values=1).ffill().dropna().reset_index().drop(columns=['index'])\n",
    "\n",
    "    #merge both dataframes\n",
    "    temp_df=pd.concat([df_temperature,df_2],axis=1)\n",
    "\n",
    "    #scrape lattitude, longitude, and elevation \n",
    "    lat_lon_elev = soup.find('span', {'class': 'smallTxt'}).text.strip()\n",
    "    lat, lon, elev = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', lat_lon_elev)\n",
    "\n",
    "    #scrape name\n",
    "    station = soup.find('h2', {'class': 'panel-title'}).text.strip()\n",
    "\n",
    "    #add location, lat, long, and elev to source_df\n",
    "    temp_df['elevation_ft'] = elev\n",
    "    temp_df['latitude'] = lat\n",
    "    temp_df['longitude'] = lon\n",
    "    temp_df['weather_station'] = station\n",
    "\n",
    "    combined_df = pd.concat([temp_df, combined_df], ignore_index=True, sort=False)\n",
    "\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIES = \"/Users/drewwhite/Desktop/Epicodus/team-week3/drew-work/airflow/data/cities.json\"\n",
    "with open(CITIES) as f:\n",
    "    CITIES = json.load(f)\n",
    "# for city in CITIES:\n",
    "#     try:\n",
    "#         print(city[\"Name\"])\n",
    "#     except:\n",
    "#         print(f\"{city} failed\")\n",
    "print(CITIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51350c9371c2ae5e4fc537f59a20b1e199e08343f95973e53e327f646c8673a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
